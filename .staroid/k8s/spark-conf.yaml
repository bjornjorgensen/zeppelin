kind: ConfigMap
apiVersion: v1
metadata:
  name: spark-initial-conf
data:
  spark-defaults.conf: |
    # This configuration is copied to persistent conf volume only if not exists (i.e. initial deploy).
    # See zeppelin-server.yaml initContainers args.
    #

    spark.dynamicAllocation.enabled                               true
    spark.dynamicAllocation.minExecutors                          1
    spark.dynamicAllocation.maxExecutors                          10
    spark.dynamicAllocation.initialExecutors                      1
    spark.dynamicAllocation.shuffleTracking.enabled               true
    spark.dynamicAllocation.executorIdleTimeout                   600s
    spark.dynamicAllocation.schedulerBacklogTimeout               60s
    spark.kubernetes.allocation.batch.size                        20
    spark.scheduler.mode                                          FAIR
    spark.serializer                                              org.apache.spark.serializer.KryoSerializer
    spark.sql.adaptive.enabled                                    true
    spark.sql.adaptive.coalescePartitions.enabled                 true

    # SKE options ---------

    # 'dedicated' for dedicated kubernetes node. 'dedicated' or 'sandboxed'
    spark.kubernetes.executor.label.pod.staroid.com/isolation     dedicated

    # 'standard-2', 'standard-4', 'standard-8' available. Note that spark.executor.cores|memory need to be changed accordingly.
    spark.kubernetes.executor.label.pod.staroid.com/instance-type standard-4

    # 'true' for spot instance. 'true' or 'false'
    spark.kubernetes.executor.label.pod.staroid.com/spot          true

    #
    # Configuration apply order is
    #
    #   Inline conf interpreter (%spark.conf) > Interpreter setting > spark-defaults.conf
    #
    # Therefore, following spark properties already defined in Interpreter settings (plus any spark property that user adds later)
    # are NOT effective in this file. They need to be changed from Interpreter Setting or conf interpreter.
    #
    # spark.executor.cores
    # spark.executor.memory
    # spark.driver.cores
    # spark.driver.memory
    # spark.app.name
    # spark.files
    # spark.jars
    # spark.jars.packages
    #
    # Default value of Interpreter setting can be changed using jvm property.
    # See 'zeppelin-server-jvm-properties' ConfigMap in zeppelin-conf.yaml.
    #
  runtime-initialize-spark-defaults.sh: |
    #!/bin/bash
    SPARK_DEFAULTS_CONF=/spark/conf/spark-defaults.conf

    # generate hive metastore connection info
    echo $HIVE_METASTORE_NAMESPACE | grep not-imported > /dev/null
    if [ $? -ne 0 ]; then
      # add a newline
      echo "" >> $SPARK_DEFAULTS_CONF
      # add generated metastore uri
      cat <<EOT >> $SPARK_DEFAULTS_CONF
    spark.hadoop.hive.metastore.uris  thrift://$HIVE_METASTORE_SERVICE.$HIVE_METASTORE_NAMESPACE:9083
    EOT
    fi
  image: opendatastudio/spark:v3.0.0
